---
title: '通过函数秘密共享实现低交互隐私保护深度学习'
date: 2023-03-15
permalink: /posts/2023/03/blog-post-7/
tags:
  - Deep Learning
  - Secure Two-party Computation
  - Function Secret Sharing
---

函数秘密共享实现低交互隐私保护深度学习

AriaNN是一种用于私有神经网络训练和敏感数据推理的低交互隐私保护框架。半诚实 2 方计算协议（与受信任的经销商）利用功能秘密共享，这是一种最近的轻量级加密协议，能够实现高效的在线阶段。 为 ReLU、MaxPool 和 BatchNorm 等神经网络的构建块设计了优化的原语。 例如，在线阶段使用输入大小的单个消息对 ReLU 操作执行私有比较，并且预处理密钥比以前的工作小近 4 倍。 AriaNN是一个扩展来支持 n 方私人联邦学习。 我们将我们的框架实现为 PyTorch 之上的可扩展系统，利用 CPU 和 GPU 硬件加速进行加密和机器学习操作。 我们评估我们的端到端系统在标准神经网络（如 AlexNet、VGG16 或 ResNet18）上的远程服务器之间的私人推理，以及在 LeNet 等较小网络上的私人训练。 

用于对敏感数据进行安全计算的密码技术的巨大改进刺激了隐私保护机器学习领域的发展。 隐私保护技术在具体用例中变得实用，因此鼓励公共当局使用它们来保护公民数据，尤其是在医疗保健应用中。然而，缺乏工具来为那些在密码学方面缺乏专业知识同时面临关键数据隐私挑战的机构提供端到端解决方案。 一个突出的例子是医院，它处理大量数据，同时拥有相对有限的技术团队。 安全多方计算 (SMPC) 是一种很有前途的技术，可以有效地集成到机器学习工作流程中以确保数据和模型隐私，同时允许多方或机构参与联合项目。 特别是，SMPC 提供了内在的共享治理：因为数据是共享的，任何一方都不能单独决定重建它。

用例是医疗机构和人工智能公司之间的协作。 医疗机构（例如医院）充当数据所有者，AI 公司充当模型所有者。 协作包括使用标记数据训练模型或使用预训练模型分析未标记数据。 培训可能涉及多个数据所有者，如第 5 节所述。由于模型可能是敏感资产（在知识产权、战略资产或监管和隐私问题方面），因此不能直接在数据所有者上进行培训（s） 使用联合学习等技术的机器：它可能被盗或被逆向工程。
我们将假设参与计算的各方位于不同的区域，并且他们可以通过网络以合理的延迟（例如 70 毫秒）传递大量信息。 这对应于广域网 (WAN) 设置，与局域网 (LAN) 设置相反，局域网 (LAN) 设置中各方通常位于同一数据中心并以低延迟（通常 <1ms）进行通信。 其次，当事人是诚实但好奇的。 因此，他们几乎没有偏离原始协议的动机，但他们会为了自己的利益使用任何可用的信息。
贡献。 通过利用函数秘密共享 (FSS)，我们提出了一个用于私有深度学习的低交互框架，它大大减少了基本机器学习操作的单轮通信，并使用 GPU 在 ResNet18 上实现了第一个私有评估基准 .
隐私保护机器学习的相关工作包括 SMPC 和完全同态加密 (FHE) 技术。
FHE 只需要单轮交互，但不支持高效的非线性。 例如，nGraph-HE及其扩展建立在 SEAL 库 之上，并提供了一个安全评估框架，大大改进了 CryptoNet 开创性工作，但它求助于多项式（ 像正方形）用于激活函数。

SMPC 框架通常使用轻量级密码学提供更快的实现。 MiniONN、DeepSecure和 XONN 使用优化的乱码电路 ，允许很少的通信轮次，但它们不支持训练和改变神经网络结构以加快执行速度。 其他框架，如 ShareMind 、SecureML、SecureNN、QUOTIENT或最近的 FALCON依赖于附加秘密共享并允许安全模型评估和训练。 他们使用更简单、更高效的原语，但需要大量的通信轮次，例如 SecureNN 中的 11 或 FALCON中的 5 + log2(n)（通常为 10，n = 32）用于 ReLU。 ABY 、Chameleon和最近的 ABY3、CrypT-Flow 和4PC根据所考虑的操作最有效的方式混合乱码电路、加法或二进制秘密共享。 但是，它们之间的转换可能很昂贵，并且它们不支持除 ABY3 之外的培训。 当前的工作包括 BLAZE、Trident和 FLASH，它们改进了 ABY3 以减少通信开销：BLAZE 和 Trident 为 ReLU 实现了例如 4 轮通信。最后，像 Gazelle这样的作品结合了 FHE 和 SMPC 以充分利用两者，但转换也可能代价高昂。在可信执行环境上的工作不在本文的讨论范围内，因为它们需要访问专用且昂贵的硬件Chiron。

Boyle 等人的并行工作改进了以前使用函数秘密共享进行私有比较的算法，并且他们的实现结果与我们的轮数相同，密钥大小也相似（大约 n(λ+n)，其中 n 是数字 编码值的位数，它考虑了正确性，通常设置为 32，λ 是安全参数，通常等于 128）。 然而，FSS不适用于机器学习：它们仅提供 ReLU 的实现，而不提供 MaxPool、BatchNorm、Argmax 或其他经典机器学习组件的实现。 此外，由于他们不提供实验基准或他们的私人比较的实施，我们无法在我们的私人 ML 框架中将其与我们的进行比较。 他们避免了我们在第 3 节中研究的可忽略的错误率，这在我们展示的机器学习环境中没有影响。

预处理由受信任的第三方在离线阶段执行，该第三方构建功能密钥并将其分发给未来计算中涉及的 2 方。 这是功能秘密共享的标准。在没有此类可信经销商的情况下，可以在输入已知之前通过离线执行的交互式安全协议生成密钥。 这种设置也可以在其他隐私保护机器学习框架中找到，包括 SecureML [46]。 这个值得信赖的经销商在在线阶段不活跃，他不知道 2 方打算执行的计算。 特别是，由于我们处于诚实但好奇的模型中，因此假设没有任何一方与经销商串通。 在实践中，此类第三方通常是一个关心其声誉的机构，并且使用剪切和选择技术可以很容易地检查预处理材料是否正确 [65]。 例如，第三方产生n个密钥进行私密比对。 愿意进行私有计算的 2 方随机检查其中的一些：他们从他们的密钥中提取 s0、s1 并从 [α]j 重建 α，j ∈ {0, 1}。 然后，他们可以推导出 KeyGen 的计算结果，并验证密钥的相关随机性是否正确。 然后他们可以使用剩余的密钥进行私有计算。

构建比较协议的基础。 然后我们描述私有比较协议，它通过专注于神经网络评估。引入的私有相等性，它比比较稍微简单一些，并提供了有关比较如何工作的有用提示。 相等性测试包括将公共输入 x 与私有值 α 进行比较。 使用功能键评估输入可以看作是遍历深度为 n 的二叉树，其中 n 是输入的位数（通常为 32）。 在所有可能的路径中，从根节点向下到 α 的路径称为特殊路径。 图 1 说明了这棵树并提供了我们的协议使用的紧凑表示，其中我们没有详细说明所有叶子都为 0 的分支。评估如下：两个评估者各自获得一个函数键，其中包括一个不同的初始随机状态 (s, t) ∈ {0, 1}λ × {0, 1}。 每个评估者从根开始，在每一步 i 沿着树中的一个节点向下移动，并使用公共校正词 CW(i) ∈ {0,1}2(λ +1) 从功能键。 在计算结束时，每个评估器输出 t。 只要 x[i] = α[i]，评估者就停留在特殊路径上，并且因为输入 x 是公开的并且对他们来说是公共的，所以他们都遵循相同的路径。 如果满足x[i] ̸= α[i]，则离开特殊路径，输出0； 否则，他们一直保持下去，这意味着 x = α，他们应该输出 1。

主要思想是当它们在特殊路径上时，评估器应该分别具有状态 (s0,t0) 和 (s1,t1)，这样 s0 和 s1 是独立同分布的。 和 $t_0 \oplus t_1 = 1$。当他们离开它时，校正词应该使 s0 = s1 但仍然与随机和 t0 = t1 无法区分，这确保 $t_0 \oplus t_1 = 0$。要以明文形式重建结果，每个评估者应该输出它的 tj，结果由 $t_0 \oplus t_1$  给出。正式描述该协议，协议由两部分组成：首先，算法 1是KeyGen 算法包括一个预处理步骤以生成功能密钥，然后，算法 2是Eval 由两个评估者运行以 执行相等性测试。 它将每个评估者持有的私人份额和他们收到的功能密钥作为输入。 他们使用 $ \mathbb{G} \colon \{ 0, 1 \}^λ → \{ 0, 1\}^{2(λ+1) }$ ，一个伪随机生成器 (PRG)，其中输出集为$ \{0, 1\}^λ+1 × \{0, 1 \}^{λ+1}$，和模 $2^n$ 的操作隐式转换回和第四位字符串转换为整数。